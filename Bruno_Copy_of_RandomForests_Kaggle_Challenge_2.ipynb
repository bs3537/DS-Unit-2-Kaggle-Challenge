{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Bruno_Copy of RandomForests_Kaggle_Challenge_2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bs3537/DS-Unit-2-Kaggle-Challenge/blob/master/Bruno_Copy_of_RandomForests_Kaggle_Challenge_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "UEHyIgGaaSEJ"
      },
      "source": [
        "Lambda School Data Science\n",
        "\n",
        "*Unit 2, Sprint 2, Module 2*\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "mRfPLX4WgLVJ"
      },
      "source": [
        "# Random Forests"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "jRRNhkxcgLVK"
      },
      "source": [
        "- use scikit-learn for **random forests**\n",
        "- do **ordinal encoding** with high-cardinality categoricals\n",
        "- understand how categorical encodings affect trees differently compared to linear models\n",
        "- understand how tree ensembles reduce overfitting compared to a single decision tree with unlimited depth"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "-3TH11e1gLVL"
      },
      "source": [
        "Today's lesson has two take-away messages:\n",
        "\n",
        "#### Try Tree Ensembles when you do machine learning with labeled, tabular data\n",
        "- \"Tree Ensembles\" means Random Forest or Gradient Boosting models. \n",
        "- [Tree Ensembles often have the best predictive accuracy](https://arxiv.org/abs/1708.05070) with labeled, tabular data.\n",
        "- Why? Because trees can fit non-linear, non-[monotonic](https://en.wikipedia.org/wiki/Monotonic_function) relationships, and [interactions](https://christophm.github.io/interpretable-ml-book/interaction.html) between features.\n",
        "- A single decision tree, grown to unlimited depth, will [overfit](http://www.r2d3.us/visual-intro-to-machine-learning-part-1/). We solve this problem by ensembling trees, with bagging (Random Forest) or boosting (Gradient Boosting).\n",
        "- Random Forest's advantage: may be less sensitive to hyperparameters. Gradient Boosting's advantage: may get better predictive accuracy.\n",
        "\n",
        "#### One-hot encoding isnâ€™t the only way, and may not be the best way, of categorical encoding for tree ensembles.\n",
        "- For example, tree ensembles can work with arbitrary \"ordinal\" encoding! (Randomly assigning an integer to each category.) Compared to one-hot encoding, the dimensionality will be lower, and the predictive accuracy may be just as good or even better.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "r5PbOFEuFfGF"
      },
      "source": [
        "### Setup\n",
        "\n",
        "Run the code cell below. You can work locally (follow the [local setup instructions](https://lambdaschool.github.io/ds/unit2/local/)) or on Colab.\n",
        "\n",
        "Libraries\n",
        "\n",
        "- **category_encoders** \n",
        "- **graphviz**\n",
        "- ipywidgets\n",
        "- matplotlib\n",
        "- numpy\n",
        "- pandas\n",
        "- seaborn\n",
        "- scikit-learn"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "FStAplyRFoEu",
        "colab": {}
      },
      "source": [
        "%%capture\n",
        "import sys\n",
        "\n",
        "# If you're on Colab:\n",
        "if 'google.colab' in sys.modules:\n",
        "    DATA_PATH = 'https://raw.githubusercontent.com/LambdaSchool/DS-Unit-2-Kaggle-Challenge/master/data/'\n",
        "    !pip install category_encoders==2.*\n",
        "\n",
        "# If you're working locally:\n",
        "else:\n",
        "    DATA_PATH = '../data/'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ZL-yK8B7gLVW"
      },
      "source": [
        "# Use scikit-learn for random forests"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "invzfPtM-GlQ",
        "colab_type": "text"
      },
      "source": [
        "## Overview\n",
        "\n",
        "Let's fit a Random Forest!\n",
        "\n",
        "![](https://pbs.twimg.com/media/EGSvKA0UUAEzUZi?format=png)\n",
        "\n",
        "[Chris Albon, MachineLearningFlashcards.com](https://twitter.com/chrisalbon/status/1181261589887909889)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "gHFxMCPSgLVM"
      },
      "source": [
        "### Solution example\n",
        "\n",
        "First, read & wrangle the data.\n",
        "\n",
        "> Define a function to wrangle train, validate, and test sets in the same way. Clean outliers and engineer features. (For example, [what other columns have zeros and shouldn't?](https://github.com/Quartz/bad-data-guide#zeros-replace-missing-values) What other columns are duplicates, or nearly duplicates? Can you extract the year from date_recorded? Can you engineer new features, such as the number of years from waterpump construction to waterpump inspection?)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "YTLm-rDagLVM",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Merge train_features.csv & train_labels.csv\n",
        "train = pd.merge(pd.read_csv(DATA_PATH+'waterpumps/train_features.csv'), \n",
        "                 pd.read_csv(DATA_PATH+'waterpumps/train_labels.csv'))\n",
        "\n",
        "# Read test_features.csv & sample_submission.csv\n",
        "test = pd.read_csv(DATA_PATH+'waterpumps/test_features.csv')\n",
        "sample_submission = pd.read_csv(DATA_PATH+'waterpumps/sample_submission.csv')\n",
        "\n",
        "# Split train into train & val\n",
        "train, val = train_test_split(train, train_size=0.80, test_size=0.20, \n",
        "                              stratify=train['status_group'], random_state=42)\n",
        "\n",
        "\n",
        "def wrangle(X):\n",
        "    \"\"\"Wrangle train, validate, and test sets in the same way\"\"\"\n",
        "    \n",
        "    # Prevent SettingWithCopyWarning\n",
        "    X = X.copy()\n",
        "    \n",
        "    # About 3% of the time, latitude has small values near zero,\n",
        "    # outside Tanzania, so we'll treat these values like zero.\n",
        "    X['latitude'] = X['latitude'].replace(-2e-08, 0)\n",
        "    \n",
        "    # When columns have zeros and shouldn't, they are like null values.\n",
        "    # So we will replace the zeros with nulls, and impute missing values later.\n",
        "    # Also create a \"missing indicator\" column, because the fact that\n",
        "    # values are missing may be a predictive signal.\n",
        "    cols_with_zeros = ['longitude', 'latitude', 'construction_year', \n",
        "                       'gps_height', 'population']\n",
        "    for col in cols_with_zeros:\n",
        "        X[col] = X[col].replace(0, np.nan)\n",
        "        X[col+'_MISSING'] = X[col].isnull()\n",
        "            \n",
        "    # Drop duplicate columns\n",
        "    duplicates = ['quantity_group', 'payment_type']\n",
        "    X = X.drop(columns=duplicates)\n",
        "    \n",
        "    # Drop recorded_by (never varies) and id (always varies, random)\n",
        "    unusable_variance = ['recorded_by', 'id']\n",
        "    X = X.drop(columns=unusable_variance)\n",
        "    \n",
        "    # Convert date_recorded to datetime\n",
        "    X['date_recorded'] = pd.to_datetime(X['date_recorded'], infer_datetime_format=True)\n",
        "    \n",
        "    # Extract components from date_recorded, then drop the original column\n",
        "    X['year_recorded'] = X['date_recorded'].dt.year\n",
        "    X['month_recorded'] = X['date_recorded'].dt.month\n",
        "    X['day_recorded'] = X['date_recorded'].dt.day\n",
        "    X = X.drop(columns='date_recorded')\n",
        "    \n",
        "    # Engineer feature: how many years from construction_year to date_recorded\n",
        "    X['years'] = X['year_recorded'] - X['construction_year']\n",
        "    X['years_MISSING'] = X['years'].isnull()\n",
        "    \n",
        "    # return the wrangled dataframe\n",
        "    return X\n",
        "\n",
        "train = wrangle(train)\n",
        "val = wrangle(val)\n",
        "test = wrangle(test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "m2HppBvZgLVP",
        "colab": {}
      },
      "source": [
        "# The status_group column is the target\n",
        "target = 'status_group'\n",
        "\n",
        "# Get a dataframe with all train columns except the target\n",
        "train_features = train.drop(columns=[target])\n",
        "\n",
        "# Get a list of the numeric features\n",
        "numeric_features = train_features.select_dtypes(include='number').columns.tolist()\n",
        "\n",
        "# Get a series with the cardinality of the nonnumeric features\n",
        "cardinality = train_features.select_dtypes(exclude='number').nunique()\n",
        "\n",
        "# Get a list of all categorical features with cardinality <= 50\n",
        "categorical_features = cardinality[cardinality <= 50].index.tolist()\n",
        "\n",
        "# Combine the lists \n",
        "features = numeric_features + categorical_features"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "aXmK2brXgLVR",
        "colab": {}
      },
      "source": [
        "# Arrange data into X features matrix and y target vector \n",
        "X_train = train[features]\n",
        "y_train = train[target]\n",
        "X_val = val[features]\n",
        "y_val = val[target]\n",
        "X_test = test[features]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PlITPAmM-Gla",
        "colab_type": "text"
      },
      "source": [
        "## Follow Along\n",
        "\n",
        "[Scikit-Learn User Guide: Random Forests](https://scikit-learn.org/stable/modules/ensemble.html#random-forests) "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "57yyygsdgLVW",
        "outputId": "2f1efef6-0b46-4760-9a3d-33cc714042ac",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "source": [
        "%%time\n",
        "\n",
        "import category_encoders as ce\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.pipeline import make_pipeline\n",
        "\n",
        "pipeline = make_pipeline(\n",
        "    ce.OneHotEncoder(use_cat_names=True), \n",
        "    SimpleImputer(strategy='median'), \n",
        "    RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)\n",
        ")\n",
        "\n",
        "# Fit on train, score on val\n",
        "pipeline.fit(X_train, y_train)\n",
        "print('Validation Accuracy', pipeline.score(X_val, y_val))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Validation Accuracy 0.8082491582491582\n",
            "CPU times: user 22.8 s, sys: 378 ms, total: 23.1 s\n",
            "Wall time: 14.4 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qfUjaa8SQepA",
        "colab_type": "code",
        "outputId": "9fa2801a-106a-4773-d3a8-754fc1c26761",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "train.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(47520, 46)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VF7EfUyELCNZ",
        "colab_type": "code",
        "outputId": "2c6305e7-af3b-4d89-f9c3-c765f648c440",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "X_train.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(47520, 38)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4UPIaCGLLMjJ",
        "colab_type": "code",
        "outputId": "9d15afd4-6389-4566-86cc-5d378b5dfd82",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "encoder = pipeline.named_steps['onehotencoder']\n",
        "encoded = encoder.transform(X_train)\n",
        "\n",
        "encoded.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(47520, 182)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S2wQZcbTLoIR",
        "colab_type": "code",
        "outputId": "40b21db4-4f60-40c7-b8c2-5c56b0172fc6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 607
        }
      },
      "source": [
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Get feature importances\n",
        "rf = pipeline.named_steps['randomforestclassifier']\n",
        "importances = pd.Series(rf.feature_importances_, encoded.columns)\n",
        "\n",
        "# Plot feature importances\n",
        "n = 20\n",
        "plt.figure(figsize=(10, n/2))\n",
        "plt.title(f'Top {n} features')\n",
        "importances.sort_values()[-n:].plot.barh(color='grey');"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAtUAAAJOCAYAAAB4EvvrAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nTkSuQmCC\n",
            "text/plain": [
              "<Figure size 720x720 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "yfyk_aa5gLVY"
      },
      "source": [
        "# Do ordinal encoding with high-cardinality categoricals"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YwdFv2em-Gle",
        "colab_type": "text"
      },
      "source": [
        "## Overview\n",
        "\n",
        "http://contrib.scikit-learn.org/categorical-encoding/ordinal.html"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "__UqshUS-Gle",
        "colab_type": "text"
      },
      "source": [
        "## Follow Along"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "b8d_WJtcgLVZ",
        "outputId": "52c01760-724c-48dd-fe02-fa89045e040b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "source": [
        "%%time\n",
        "\n",
        "# Arrange data to include all categorical features and not ignore those > 50\n",
        "X_train = train.drop(columns=target)\n",
        "y_train = train[target]\n",
        "X_val = val.drop(columns=target)\n",
        "y_val = val[target]\n",
        "X_test = test\n",
        "\n",
        "# pipeline, identical to above but with ordinal encoder\n",
        "pipeline = make_pipeline(\n",
        "    ce.OneHotEncoder(use_cat_names=True, cols=['basin']), \n",
        "    ce.OrdinalEncoder(), \n",
        "    SimpleImputer(strategy='most_frequent'), \n",
        "    RandomForestClassifier(n_estimators=2000, n_jobs=-1, random_state=42)\n",
        ")\n",
        "\n",
        "# Fit on train, score on val\n",
        "pipeline.fit(X_train, y_train)\n",
        "print('Validation Accuracy', pipeline.score(X_val, y_val))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Validation Accuracy 0.8138888888888889\n",
            "CPU times: user 5min 42s, sys: 5.58 s, total: 5min 48s\n",
            "Wall time: 2min 57s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yE-5U9-jUZQH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Predict on Test Data\n",
        "y_pred = pipeline.predict(X_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WZqa0c8TUyZI",
        "colab_type": "code",
        "outputId": "454d7a00-04b3-428a-a642-c0295e49d494",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "y_pred"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['non functional', 'functional', 'functional', ..., 'functional',\n",
              "       'functional', 'non functional'], dtype=object)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sTgdy0caVDuY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "sample_submission = pd.read_csv(DATA_PATH+'waterpumps/sample_submission.csv')\n",
        "submission = sample_submission.copy()\n",
        "submission['status_group'] = y_pred\n",
        "submission.to_csv('23_59_bhav_tanzania_randomforest_', index=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5aQXdgH6RO62",
        "colab_type": "code",
        "outputId": "de7bf616-12f1-47fe-de7c-086cf741f4a0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 335
        }
      },
      "source": [
        "encoder = pipeline.named_steps['ordinalencoder']\n",
        "encoded = encoder.transform(X_train)\n",
        "encoded.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-d251b7c8c5f0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mencoder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpipeline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnamed_steps\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'ordinalencoder'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mencoded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mencoded\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/category_encoders/ordinal.py\u001b[0m in \u001b[0;36mtransform\u001b[0;34m(self, X, override_return_df)\u001b[0m\n\u001b[1;32m    193\u001b[0m         \u001b[0;31m# then make sure that it is the right size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dim\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 195\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Unexpected input dimension %d, expected %d'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dim\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    196\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcols\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Unexpected input dimension 45, expected 53"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CnUP619FRXsP",
        "colab_type": "code",
        "outputId": "8123f85c-6769-4c6b-84d3-85c014a8272f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        }
      },
      "source": [
        "encoded.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>amount_tsh</th>\n",
              "      <th>gps_height</th>\n",
              "      <th>longitude</th>\n",
              "      <th>latitude</th>\n",
              "      <th>num_private</th>\n",
              "      <th>region_code</th>\n",
              "      <th>district_code</th>\n",
              "      <th>population</th>\n",
              "      <th>construction_year</th>\n",
              "      <th>year_recorded</th>\n",
              "      <th>month_recorded</th>\n",
              "      <th>day_recorded</th>\n",
              "      <th>years</th>\n",
              "      <th>basin_Lake Nyasa</th>\n",
              "      <th>basin_Rufiji</th>\n",
              "      <th>basin_Wami / Ruvu</th>\n",
              "      <th>basin_Lake Victoria</th>\n",
              "      <th>basin_Internal</th>\n",
              "      <th>basin_Lake Tanganyika</th>\n",
              "      <th>basin_Pangani</th>\n",
              "      <th>basin_Lake Rukwa</th>\n",
              "      <th>basin_Ruvuma / Southern Coast</th>\n",
              "      <th>region_Mbeya</th>\n",
              "      <th>region_Iringa</th>\n",
              "      <th>region_Pwani</th>\n",
              "      <th>region_Kagera</th>\n",
              "      <th>region_Dodoma</th>\n",
              "      <th>region_Rukwa</th>\n",
              "      <th>region_Arusha</th>\n",
              "      <th>region_Mwanza</th>\n",
              "      <th>region_Mtwara</th>\n",
              "      <th>region_Tanga</th>\n",
              "      <th>region_Kilimanjaro</th>\n",
              "      <th>region_Manyara</th>\n",
              "      <th>region_Lindi</th>\n",
              "      <th>region_Kigoma</th>\n",
              "      <th>region_Morogoro</th>\n",
              "      <th>region_Shinyanga</th>\n",
              "      <th>region_Ruvuma</th>\n",
              "      <th>region_Tabora</th>\n",
              "      <th>...</th>\n",
              "      <th>quantity_unknown</th>\n",
              "      <th>source_spring</th>\n",
              "      <th>source_shallow well</th>\n",
              "      <th>source_machine dbh</th>\n",
              "      <th>source_river</th>\n",
              "      <th>source_hand dtw</th>\n",
              "      <th>source_lake</th>\n",
              "      <th>source_rainwater harvesting</th>\n",
              "      <th>source_dam</th>\n",
              "      <th>source_other</th>\n",
              "      <th>source_unknown</th>\n",
              "      <th>source_type_spring</th>\n",
              "      <th>source_type_shallow well</th>\n",
              "      <th>source_type_borehole</th>\n",
              "      <th>source_type_river/lake</th>\n",
              "      <th>source_type_rainwater harvesting</th>\n",
              "      <th>source_type_dam</th>\n",
              "      <th>source_type_other</th>\n",
              "      <th>source_class_groundwater</th>\n",
              "      <th>source_class_surface</th>\n",
              "      <th>source_class_unknown</th>\n",
              "      <th>waterpoint_type_communal standpipe</th>\n",
              "      <th>waterpoint_type_hand pump</th>\n",
              "      <th>waterpoint_type_other</th>\n",
              "      <th>waterpoint_type_communal standpipe multiple</th>\n",
              "      <th>waterpoint_type_improved spring</th>\n",
              "      <th>waterpoint_type_cattle trough</th>\n",
              "      <th>waterpoint_type_dam</th>\n",
              "      <th>waterpoint_type_group_communal standpipe</th>\n",
              "      <th>waterpoint_type_group_hand pump</th>\n",
              "      <th>waterpoint_type_group_other</th>\n",
              "      <th>waterpoint_type_group_improved spring</th>\n",
              "      <th>waterpoint_type_group_cattle trough</th>\n",
              "      <th>waterpoint_type_group_dam</th>\n",
              "      <th>longitude_MISSING</th>\n",
              "      <th>latitude_MISSING</th>\n",
              "      <th>construction_year_MISSING</th>\n",
              "      <th>gps_height_MISSING</th>\n",
              "      <th>population_MISSING</th>\n",
              "      <th>years_MISSING</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>43360</th>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>33.542898</td>\n",
              "      <td>-9.174777</td>\n",
              "      <td>0</td>\n",
              "      <td>12</td>\n",
              "      <td>4</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2011</td>\n",
              "      <td>7</td>\n",
              "      <td>27</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>True</td>\n",
              "      <td>True</td>\n",
              "      <td>True</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7263</th>\n",
              "      <td>500.0</td>\n",
              "      <td>2049.0</td>\n",
              "      <td>34.665760</td>\n",
              "      <td>-9.308548</td>\n",
              "      <td>0</td>\n",
              "      <td>11</td>\n",
              "      <td>4</td>\n",
              "      <td>175.0</td>\n",
              "      <td>2008.0</td>\n",
              "      <td>2011</td>\n",
              "      <td>3</td>\n",
              "      <td>23</td>\n",
              "      <td>3.0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2486</th>\n",
              "      <td>25.0</td>\n",
              "      <td>290.0</td>\n",
              "      <td>38.238568</td>\n",
              "      <td>-6.179919</td>\n",
              "      <td>0</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>2300.0</td>\n",
              "      <td>2010.0</td>\n",
              "      <td>2011</td>\n",
              "      <td>3</td>\n",
              "      <td>7</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>313</th>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>30.716727</td>\n",
              "      <td>-1.289055</td>\n",
              "      <td>0</td>\n",
              "      <td>18</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2011</td>\n",
              "      <td>7</td>\n",
              "      <td>31</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>True</td>\n",
              "      <td>True</td>\n",
              "      <td>True</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>52726</th>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>35.389331</td>\n",
              "      <td>-6.399942</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2011</td>\n",
              "      <td>3</td>\n",
              "      <td>10</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>True</td>\n",
              "      <td>True</td>\n",
              "      <td>True</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows Ã— 182 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "       amount_tsh  gps_height  ...  population_MISSING  years_MISSING\n",
              "43360         0.0         NaN  ...                True           True\n",
              "7263        500.0      2049.0  ...               False          False\n",
              "2486         25.0       290.0  ...               False          False\n",
              "313           0.0         NaN  ...                True           True\n",
              "52726         0.0         NaN  ...                True           True\n",
              "\n",
              "[5 rows x 182 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "clNjGE8oRvq1",
        "colab_type": "code",
        "outputId": "839f5b1c-c3d1-4f64-a610-f456b2521424",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 371
        }
      },
      "source": [
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Get feature importances\n",
        "rf = pipeline.named_steps['randomforestclassifier']\n",
        "importances = pd.Series(rf.feature_importances_, encoded.columns)\n",
        "\n",
        "# Plot feature importances\n",
        "n = 20\n",
        "plt.figure(figsize=(10, n/2))\n",
        "plt.title(f'Top {n} features')\n",
        "importances.sort_values()[-n:].plot.barh(color='grey');"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-7e360eda8ded>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Get feature importances\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mrf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpipeline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnamed_steps\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'randomforestclassifier'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mimportances\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSeries\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_importances_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoded\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# Plot feature importances\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/series.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, index, dtype, name, copy, fastpath)\u001b[0m\n\u001b[1;32m    300\u001b[0m                         raise ValueError(\n\u001b[1;32m    301\u001b[0m                             \u001b[0;34m\"Length of passed values is {val}, \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 302\u001b[0;31m                             \u001b[0;34m\"index implies {ind}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mind\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    303\u001b[0m                         )\n\u001b[1;32m    304\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Length of passed values is 53, index implies 182"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "xs2UPoVdgLVp"
      },
      "source": [
        "# Understand how categorical encodings affect trees differently compared to linear models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wLsAF-DV-Gli",
        "colab_type": "text"
      },
      "source": [
        "## Follow Along"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "z8V-A92mgLVp"
      },
      "source": [
        "### Categorical exploration, 1 feature at a time\n",
        "\n",
        "Change `feature`, then re-run these cells!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "G35RAzVdgLVq",
        "colab": {}
      },
      "source": [
        "feature = 'extraction_type_class'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "OuxHWiH8gLVr",
        "colab": {}
      },
      "source": [
        "X_train[feature].value_counts()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "pVxoC4NngLVt",
        "colab": {}
      },
      "source": [
        "import seaborn as sns\n",
        "plt.figure(figsize=(16,9))\n",
        "sns.barplot(\n",
        "    x=train[feature], \n",
        "    y=train['status_group']=='functional', \n",
        "    color='grey'\n",
        ");"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "w99mek14gLVv",
        "colab": {}
      },
      "source": [
        "X_train[feature].head(20)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ezzK2IdbgLVx"
      },
      "source": [
        "### [One Hot Encoding](http://contrib.scikit-learn.org/categorical-encoding/onehot.html)\n",
        "\n",
        "> Onehot (or dummy) coding for categorical features, produces one feature per category, each binary.\n",
        "\n",
        "Warning: May run slow, or run out of memory, with high cardinality categoricals!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "HDQZtV6GgLVy",
        "colab": {}
      },
      "source": [
        "encoder = ce.OneHotEncoder(use_cat_names=True)\n",
        "encoded = encoder.fit_transform(X_train[[feature]])\n",
        "print(f'{len(encoded.columns)} columns')\n",
        "encoded.head(20)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "1Ql9Qmw3sNJ7"
      },
      "source": [
        "#### One-Hot Encoding, Logistic Regression, Validation Accuracy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "mT4A-oDGpOss",
        "colab": {}
      },
      "source": [
        "from sklearn.linear_model import LogisticRegressionCV\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "lr = make_pipeline(\n",
        "    ce.OneHotEncoder(use_cat_names=True), \n",
        "    SimpleImputer(), \n",
        "    StandardScaler(), \n",
        "    LogisticRegressionCV(multi_class='auto', solver='lbfgs', cv=5, n_jobs=-1)\n",
        ")\n",
        "\n",
        "lr.fit(X_train[[feature]], y_train)\n",
        "score = lr.score(X_val[[feature]], y_val)\n",
        "print('Logistic Regression, Validation Accuracy', score)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "EbH6wivpsRuV"
      },
      "source": [
        "#### One-Hot Encoding, Decision Tree, Validation Accuracy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "b6KUluFOqIdK",
        "colab": {}
      },
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "dt = make_pipeline(\n",
        "    ce.OneHotEncoder(use_cat_names=True), \n",
        "    SimpleImputer(), \n",
        "    DecisionTreeClassifier(random_state=42)\n",
        ")\n",
        "\n",
        "dt.fit(X_train[[feature]], y_train)\n",
        "score = dt.score(X_val[[feature]], y_val)\n",
        "print('Decision Tree, Validation Accuracy', score)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "8yg11_gTsUu6"
      },
      "source": [
        "#### One-Hot Encoding, Logistic Regression, Model Interpretation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "IxHwXGRornNI",
        "colab": {}
      },
      "source": [
        "model = lr.named_steps['logisticregressioncv']\n",
        "encoder = lr.named_steps['onehotencoder']\n",
        "encoded_columns = encoder.transform(X_val[[feature]]).columns\n",
        "coefficients = pd.Series(model.coef_[0], encoded_columns)\n",
        "coefficients.sort_values().plot.barh(color='grey');"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "0REZ8HdpsccR"
      },
      "source": [
        "#### One-Hot Encoding, Decision Tree, Model Interpretation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "gV-grmYKpDp9",
        "colab": {}
      },
      "source": [
        "# Plot tree\n",
        "# https://scikit-learn.org/stable/modules/generated/sklearn.tree.export_graphviz.html\n",
        "import graphviz\n",
        "from sklearn.tree import export_graphviz\n",
        "\n",
        "model = dt.named_steps['decisiontreeclassifier']\n",
        "encoder = dt.named_steps['onehotencoder']\n",
        "encoded_columns = encoder.transform(X_val[[feature]]).columns\n",
        "\n",
        "dot_data = export_graphviz(model, \n",
        "                           out_file=None, \n",
        "                           max_depth=7, \n",
        "                           feature_names=encoded_columns,\n",
        "                           class_names=model.classes_, \n",
        "                           impurity=False, \n",
        "                           filled=True, \n",
        "                           proportion=True, \n",
        "                           rounded=True)   \n",
        "display(graphviz.Source(dot_data))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "QUd6gzcZgLVz"
      },
      "source": [
        "### [Ordinal Encoding](http://contrib.scikit-learn.org/categorical-encoding/ordinal.html)\n",
        "\n",
        "> Ordinal encoding uses a single column of integers to represent the classes. An optional mapping dict can be passed in; in this case, we use the knowledge that there is some true order to the classes themselves. Otherwise, the classes are assumed to have no true order and integers are selected at random."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "CnBz2RbwgLVz",
        "colab": {}
      },
      "source": [
        "encoder = ce.OrdinalEncoder()\n",
        "encoded = encoder.fit_transform(X_train[[feature]])\n",
        "print(f'1 column, {encoded[feature].nunique()} unique values')\n",
        "encoded.head(20)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Nd-ZWprasqUM"
      },
      "source": [
        "#### Ordinal Encoding, Logistic Regression, Validation Accuracy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "GJ1YpwjvrhfL",
        "colab": {}
      },
      "source": [
        "lr = make_pipeline(\n",
        "    ce.OrdinalEncoder(), \n",
        "    SimpleImputer(), \n",
        "    StandardScaler(), \n",
        "    LogisticRegressionCV(multi_class='auto', solver='lbfgs', cv=5, n_jobs=-1)\n",
        ")\n",
        "\n",
        "lr.fit(X_train[[feature]], y_train)\n",
        "score = lr.score(X_val[[feature]], y_val)\n",
        "print('Logistic Regression, Validation Accuracy', score)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "9lO_R3SksuHs"
      },
      "source": [
        "#### Ordinal Encoding, Decision Tree, Validation Accuracy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "aOELD_roriVI",
        "colab": {}
      },
      "source": [
        "dt = make_pipeline(\n",
        "    ce.OrdinalEncoder(), \n",
        "    SimpleImputer(), \n",
        "    DecisionTreeClassifier(random_state=42)\n",
        ")\n",
        "\n",
        "dt.fit(X_train[[feature]], y_train)\n",
        "score = dt.score(X_val[[feature]], y_val)\n",
        "print('Decision Tree, Validation Accuracy', score)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "7V2zHjiwswTg"
      },
      "source": [
        "#### Ordinal Encoding, Logistic Regression, Model Interpretation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "S9UPYPois8QR",
        "colab": {}
      },
      "source": [
        "model = lr.named_steps['logisticregressioncv']\n",
        "encoder = lr.named_steps['ordinalencoder']\n",
        "encoded_columns = encoder.transform(X_val[[feature]]).columns\n",
        "coefficients = pd.Series(model.coef_[0], encoded_columns)\n",
        "coefficients.sort_values().plot.barh(color='grey');"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "MvmmvE8fsymh"
      },
      "source": [
        "#### Ordinal Encoding, Decision Tree, Model Interpretation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "jCvUu4Oms88b",
        "colab": {}
      },
      "source": [
        "model = dt.named_steps['decisiontreeclassifier']\n",
        "encoder = dt.named_steps['ordinalencoder']\n",
        "encoded_columns = encoder.transform(X_val[[feature]]).columns\n",
        "\n",
        "dot_data = export_graphviz(model, \n",
        "                           out_file=None, \n",
        "                           max_depth=5, \n",
        "                           feature_names=encoded_columns,\n",
        "                           class_names=model.classes_, \n",
        "                           impurity=False, \n",
        "                           filled=True, \n",
        "                           proportion=True, \n",
        "                           rounded=True)   \n",
        "display(graphviz.Source(dot_data))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "P4EJi2GvgLVa"
      },
      "source": [
        "# Understand how tree ensembles reduce overfitting compared to a single decision tree with unlimited depth"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5IcA9FRW-GmW",
        "colab_type": "text"
      },
      "source": [
        "## Overview"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "0nNABF3HgLVg"
      },
      "source": [
        "### What's \"random\" about random forests?\n",
        "1. Each tree trains on a random bootstrap sample of the data. (In scikit-learn, for `RandomForestRegressor` and `RandomForestClassifier`, the `bootstrap` parameter's default is `True`.) This type of ensembling is called Bagging. (Bootstrap AGGregatING.)\n",
        "2. Each split considers a random subset of the features. (In scikit-learn, when the `max_features` parameter is not `None`.) \n",
        "\n",
        "For extra randomness, you can try [\"extremely randomized trees\"](https://scikit-learn.org/stable/modules/ensemble.html#extremely-randomized-trees)!\n",
        "\n",
        ">In extremely randomized trees (see [ExtraTreesClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.ExtraTreesClassifier.html) and [ExtraTreesRegressor](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.ExtraTreesRegressor.html) classes), randomness goes one step further in the way splits are computed. As in random forests, a random subset of candidate features is used, but instead of looking for the most discriminative thresholds, thresholds are drawn at random for each candidate feature and the best of these randomly-generated thresholds is picked as the splitting rule. This usually allows to reduce the variance of the model a bit more, at the expense of a slightly greater increase in bias"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-__GxJ8Q-GmX",
        "colab_type": "text"
      },
      "source": [
        "## Follow Along"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "pUYP619CgLVb"
      },
      "source": [
        "### Example: [predicting golf putts](https://statmodeling.stat.columbia.edu/2008/12/04/the_golf_puttin/)\n",
        "(1 feature, non-linear, regression)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "b4640ukxgLVc",
        "colab": {}
      },
      "source": [
        "putts = pd.DataFrame(\n",
        "    columns=['distance', 'tries', 'successes'], \n",
        "    data = [[2, 1443, 1346],\n",
        "            [3, 694, 577],\n",
        "            [4, 455, 337],\n",
        "            [5, 353, 208],\n",
        "            [6, 272, 149],\n",
        "            [7, 256, 136],\n",
        "            [8, 240, 111],\n",
        "            [9, 217, 69],\n",
        "            [10, 200, 67],\n",
        "            [11, 237, 75],\n",
        "            [12, 202, 52],\n",
        "            [13, 192, 46],\n",
        "            [14, 174, 54],\n",
        "            [15, 167, 28],\n",
        "            [16, 201, 27],\n",
        "            [17, 195, 31],\n",
        "            [18, 191, 33],\n",
        "            [19, 147, 20],\n",
        "            [20, 152, 24]]\n",
        ")\n",
        "\n",
        "putts['rate of success'] = putts['successes'] / putts['tries']\n",
        "putts_X = putts[['distance']]\n",
        "putts_y = putts['rate of success']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "T0IpCcKggLVd",
        "colab": {}
      },
      "source": [
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "from ipywidgets import interact\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "\n",
        "def putt_trees(max_depth=1, n_estimators=1):\n",
        "    models = [DecisionTreeRegressor(max_depth=max_depth), \n",
        "              RandomForestRegressor(max_depth=max_depth, n_estimators=n_estimators)]\n",
        "    \n",
        "    for model in models:\n",
        "        name = model.__class__.__name__\n",
        "        model.fit(putts_X, putts_y)\n",
        "        ax = putts.plot('distance', 'rate of success', kind='scatter', title=name)\n",
        "        ax.step(putts_X, model.predict(putts_X), where='mid')\n",
        "        plt.show()\n",
        "        \n",
        "interact(putt_trees, max_depth=(1,6,1), n_estimators=(10,40,10));"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "_KgZK9_9gLVh"
      },
      "source": [
        "### Bagging demo, with golf putts data\n",
        "https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.sample.html"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "vA9mrSTNgLVi",
        "scrolled": false,
        "colab": {}
      },
      "source": [
        "# Do-it-yourself Bagging Ensemble of Decision Trees (like a Random Forest)\n",
        "def diy_bagging(max_depth=1, n_estimators=1):\n",
        "    y_preds = []\n",
        "    for i in range(n_estimators):\n",
        "        title = f'Tree {i+1}'\n",
        "        bootstrap_sample = putts.sample(n=len(putts), replace=True).sort_values(by='distance')\n",
        "        bootstrap_X = bootstrap_sample[['distance']]\n",
        "        bootstrap_y = bootstrap_sample['rate of success']\n",
        "        tree = DecisionTreeRegressor(max_depth=max_depth)\n",
        "        tree.fit(bootstrap_X, bootstrap_y)\n",
        "        y_pred = tree.predict(bootstrap_X)\n",
        "        y_preds.append(y_pred)\n",
        "        ax = bootstrap_sample.plot('distance', 'rate of success', kind='scatter', title=title)\n",
        "        ax.step(bootstrap_X, y_pred, where='mid')\n",
        "        plt.show()\n",
        "        \n",
        "    ensembled = np.vstack(y_preds).mean(axis=0)\n",
        "    title = f'Ensemble of {n_estimators} trees, with max_depth={max_depth}'\n",
        "    ax = putts.plot('distance', 'rate of success', kind='scatter', title=title)\n",
        "    ax.step(putts_X, ensembled, where='mid')\n",
        "    plt.show()\n",
        "    \n",
        "interact(diy_bagging, max_depth=(1,6,1), n_estimators=(2,5,1));"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "rq4Z_wQ_gLVj"
      },
      "source": [
        "### Go back to Tanzania Waterpumps ..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "FoSE9iT6YXQz"
      },
      "source": [
        "#### Helper function to visualize predicted probabilities\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "HzIAjGpJgLVj",
        "colab": {}
      },
      "source": [
        "import itertools\n",
        "import seaborn as sns\n",
        "\n",
        "def pred_heatmap(model, X, features, class_index=-1, title='', num=100):\n",
        "    \"\"\"\n",
        "    Visualize predicted probabilities, for classifier fit on 2 numeric features\n",
        "    \n",
        "    Parameters\n",
        "    ----------\n",
        "    model : scikit-learn classifier, already fit\n",
        "    X : pandas dataframe, which was used to fit model\n",
        "    features : list of strings, column names of the 2 numeric features\n",
        "    class_index : integer, index of class label\n",
        "    title : string, title of plot\n",
        "    num : int, number of grid points for each feature\n",
        "    \n",
        "    Returns\n",
        "    -------\n",
        "    y_pred_proba : numpy array, predicted probabilities for class_index\n",
        "    \"\"\"\n",
        "    feature1, feature2 = features\n",
        "    min1, max1 = X[feature1].min(), X[feature1].max()\n",
        "    min2, max2 = X[feature2].min(), X[feature2].max()\n",
        "    x1 = np.linspace(min1, max1, num)\n",
        "    x2 = np.linspace(max2, min2, num)\n",
        "    combos = list(itertools.product(x1, x2))\n",
        "    y_pred_proba = model.predict_proba(combos)[:, class_index]\n",
        "    pred_grid = y_pred_proba.reshape(num, num).T\n",
        "    table = pd.DataFrame(pred_grid, columns=x1, index=x2)\n",
        "    sns.heatmap(table, vmin=0, vmax=1)\n",
        "    plt.xticks([])\n",
        "    plt.yticks([])\n",
        "    plt.xlabel(feature1)\n",
        "    plt.ylabel(feature2)\n",
        "    plt.title(title)\n",
        "    plt.show()\n",
        "    return y_pred_proba\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "DiRfPqHjgLVl"
      },
      "source": [
        "### Compare Decision Tree, Random Forest, Logistic Regression"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "HKkMLXhMgLVl",
        "colab": {}
      },
      "source": [
        "# Instructions\n",
        "# 1. Choose two features\n",
        "# 2. Run this code cell\n",
        "# 3. Interact with the widget sliders\n",
        "feature1 = 'longitude'\n",
        "feature2 = 'quantity'\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "def get_X_y(df, feature1, feature2, target):\n",
        "    features = [feature1, feature2]\n",
        "    X = df[features]\n",
        "    y = df[target]\n",
        "    X = X.fillna(X.median())\n",
        "    X = ce.OrdinalEncoder().fit_transform(X)\n",
        "    return X, y\n",
        "\n",
        "def compare_models(max_depth=1, n_estimators=1):\n",
        "    models = [DecisionTreeClassifier(max_depth=max_depth), \n",
        "              RandomForestClassifier(max_depth=max_depth, n_estimators=n_estimators), \n",
        "              LogisticRegression(solver='lbfgs', multi_class='auto')]\n",
        "    \n",
        "    for model in models:\n",
        "        name = model.__class__.__name__\n",
        "        model.fit(X, y)\n",
        "        pred_heatmap(model, X, [feature1, feature2], class_index=0, title=name)\n",
        "\n",
        "X, y = get_X_y(train, feature1, feature2, target='status_group')\n",
        "interact(compare_models, max_depth=(1,6,1), n_estimators=(10,40,10));"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "hOQqjLEDgLVn"
      },
      "source": [
        "### Bagging"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Hm4aPgs2gLVn",
        "colab": {}
      },
      "source": [
        "# Do-it-yourself Bagging Ensemble of Decision Trees (like a Random Forest)\n",
        "\n",
        "# Instructions\n",
        "# 1. Choose two features\n",
        "# 2. Run this code cell\n",
        "# 3. Interact with the widget sliders\n",
        "\n",
        "feature1 = 'longitude'\n",
        "feature2 = 'latitude'\n",
        "\n",
        "def waterpumps_bagging(max_depth=1, n_estimators=1):\n",
        "    predicteds = []\n",
        "    for i in range(n_estimators):\n",
        "        title = f'Tree {i+1}'\n",
        "        bootstrap_sample = train.sample(n=len(train), replace=True)\n",
        "        X, y = get_X_y(bootstrap_sample, feature1, feature2, target='status_group')\n",
        "        tree = DecisionTreeClassifier(max_depth=max_depth)\n",
        "        tree.fit(X, y)\n",
        "        predicted = pred_heatmap(tree, X, [feature1, feature2], class_index=0, title=title)\n",
        "        predicteds.append(predicted)\n",
        "    \n",
        "    ensembled = np.vstack(predicteds).mean(axis=0)\n",
        "    title = f'Ensemble of {n_estimators} trees, with max_depth={max_depth}'\n",
        "    sns.heatmap(ensembled.reshape(100, 100).T, vmin=0, vmax=1)\n",
        "    plt.title(title)\n",
        "    plt.xlabel(feature1)\n",
        "    plt.ylabel(feature2)\n",
        "    plt.xticks([])\n",
        "    plt.yticks([])\n",
        "    plt.show()\n",
        "        \n",
        "interact(waterpumps_bagging, max_depth=(1,6,1), n_estimators=(2,5,1));"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "wYoSBi15akWP"
      },
      "source": [
        "# Review\n",
        "\n",
        "#### Try Tree Ensembles when you do machine learning with labeled, tabular data\n",
        "- \"Tree Ensembles\" means Random Forest or Gradient Boosting models. \n",
        "- [Tree Ensembles often have the best predictive accuracy](https://arxiv.org/abs/1708.05070) with labeled, tabular data.\n",
        "- Why? Because trees can fit non-linear, non-[monotonic](https://en.wikipedia.org/wiki/Monotonic_function) relationships, and [interactions](https://christophm.github.io/interpretable-ml-book/interaction.html) between features.\n",
        "- A single decision tree, grown to unlimited depth, will [overfit](http://www.r2d3.us/visual-intro-to-machine-learning-part-1/). We solve this problem by ensembling trees, with bagging (Random Forest) or boosting (Gradient Boosting).\n",
        "- Random Forest's advantage: may be less sensitive to hyperparameters. Gradient Boosting's advantage: may get better predictive accuracy.\n",
        "\n",
        "#### One-hot encoding isnâ€™t the only way, and may not be the best way, of categorical encoding for tree ensembles.\n",
        "- For example, tree ensembles can work with arbitrary \"ordinal\" encoding! (Randomly assigning an integer to each category.) Compared to one-hot encoding, the dimensionality will be lower, and the predictive accuracy may be just as good or even better.\n"
      ]
    }
  ]
}